\chapter{Conclusions and Suggestions for Future Work}

We have implemented the PIMC method with two approaches based on semi-classical beads (SCB-QFH*,SCB-TI) and MSMC to compute more precise quantum virial coefficients for helium-4. The SCB results agree well with CB results as they are within statistical uncertainties of each other. The decomposition algorithm of Shaul et al.~\cite{Shaul2012} was implemented to achieve better efficiency of quantum virial coefficient calculations. We observed similar trends in decompositions of simulations in our SCB based approaches as was the case for the CB approach. For lower temperatures, the approximation $u^{\rm simple}$ to $u$ for finite $P$ is chosen as the preliminary approximation. As the temperature increases, the preliminary approximation preferred is the semi-classical approximation to $u^{\rm simple}$, and for high temperatures the semi-classical approximation to $u$ is preferred. Having chosen the preliminary approximation, the decomposition algorithm spends maximum time in the first step and the amount of time spent per step gradually decreases for subsequent steps. This is because the subsequent steps involve more computationally expensive calculations (either by shifting to $u$ from its semi-classical approximation, or by doubling $P$ from the previous step, or by shifting to the full potential $u$ from $u^{\rm simple}$) and by design, these steps also yield better and better precision. The decomposition algorithm was designed to allocate computational effort proportional to the difficulty of the computation, which is defined as (error~$\times$~uncertainty). The SCB-QFH* and SCB-TI approaches have comparable and better uncertainties respectively, for the steps that involve computing $[\Gamma(P,u) - \Gamma(P/2,u)]$ or $[\Gamma(P,u^{\rm simple}) - \Gamma(P/2,u^{\rm simple})]$. Since these steps involve significant computational costs and relatively low uncertainties, the amount of effort dedicated for them is lower, attenuating the effect of any efficiency brought to their calculation. As a result, the improvement of the precision of the resulting virial coefficient is only marginal. We note that if the decomposition algorithm is not being used, either because it is non-trivial to apply, or because virial coefficients are not being computed, the SCB-TI approach performs much better than both SCB-QFH* and CB approaches, which is what we would expect anyway from the use of a higher order propagator.

In summary, we found the following order for the rate of convergence with respect to number of beads $P$: SCB-TI $>$ SCB-QFH* $>$ CB. We expect a similar trend for the rate of convergence with respect to $P$ for higher order coefficients as well, because of the use of the higher order TI propagator. The order for precision was found to be: SCB-TI $>$ SCB-QFH*. Compared to CB, QFH* is always worse but only marginally so; TI is almost always better and only marginally worse for a few temperatures. We expected a trend similar to the rate of convergence with $P$ for the precision as well, even for $B_2$ calculations. Since this was not what we observed, partially due to the decomposition algorithm, an understanding of the order of precision for higher order coefficients for the SCB based approaches compared to CB approach would require further investigation. However, we do expect the order between SCB based approaches to remain the same, i.e., SCB-TI $>$ SCB-QFH*.

Directions for future work include investigating more temperatures, comparing the performance of different higher-order propagators of the thermal density matrix in terms of precision and rate of convergence, and using alternative \emph{ab initio} potentials as they become available.
    %The empirically-observed correctness of the QFH* potential in the PIMC framework suggests that it might be possible to improve the precision and achieve faster convergence from an \emph{ad hoc} effective potential without having to use an expression for the higher order propagators for the thermal density matrix. In any case, investigating the relationship between an \emph{ad hoc} potential and the higher order propagator will provide further insight into constructing better potentials.
Extension of PIMC with semi-classical beads to multi-atomic molecules is straightforward, and we expect such an approach to perform better than conventional PIMC with classical beads, in terms of convergence rate and precision.
We have developed orientation and bond length sampling algorithms for diatomic molecules by treating each molecule as two independent atoms as opposed to one rigid rotor. This description proves to be advantageous as it not only saves time, but also provides a straightforward route for extending the algorithms to multiatomic molecules. The algorithms involve simple and analytic expressions for probability distributions that are approximations to the actual distributions. The nature of different assumptions made for both algorithms cause an adverse effect on the efficiency for large $P$ (typically at low temperatures) but, due to the use of the Monte Carlo sampling algorithm, the assumptions made to estimate the probability distribution does not impact the accuracy of the calculations. A possible route to improvement in efficiency for higher $P$, applicable to both rigid and flexible cases, is be to regrow only parts of the ring in a Monte Carlo trial. Also, more efficient ways to handle the possibility of different images having different bond lengths within the orientation move, and different images having different orientations within the bond length move, could further improve the performance of these algorithms. One could do more analysis (as was done in sec. \ref{sec:orPerformance}) using actual simulation parameters to gain a better understanding of the nature of difference between the approximate and actual distributions. Although such an analysis was not attempted for the case of the bond length trial for the test case of H$_2$, we suspect that these might become necessary in the future for other molecules, where the performance of the bond length sampling algorithm might not be considered satisfactory. Nevertheless, we have demonstrated these algorithms by computing and comparing second virial coefficients for H$_2$ as a test case. We observed \hl{good agreement} of our results with those available in literature for the three different cases explained in sec. \ref{sec:Computational details}. For lower temperatures ($T \le 100 K$), we concluded that a greater number of images $P$ were required to achieve results in better agreement with literature. We emphasize that the failure to converge using insufficient $P$ is an artifact of quantum mechanics and not the orientation sampling algorithm. Future work should be directed to extending these approaches to multiatomic molecules like H$_2$O and address some of the existing pitfalls in both the algorithms.
We have calculated fully quantum virial coefficients for the nitrogen and oxygen dimers by explicitly including nuclear quantum effects via PIMC method. Additionally, we have calculated fully quantum virial coefficients for the nitrogen dimer using PIMC with semi-classical beads, an approach that was recently (cite:helium) introduced. Based on our results, we have concluded that for the purpose of quantum second virial coefficient calculations, the utilization of semi-classical beads does not provide any significant improvement over the classical beads in terms of faster convergence and better precision. We have also employed a new algorithm that we recently (cite:hydrogen) developed to sample orientations of diatomic molecules, in all our PIMC calculations. We have compared our PIMC results against the most accurate data available in literature and found excellent agreement for the both dimers. As a result of this work, the validity and robustness of the orientation sampling algorithm have been successfully tested.

In the future, we would like to develop similar algorithms for sampling orientations of triatomic non-linear molecules like water. Also, further analysis into the current algorithm might help identify new sources of computational efficiency. A rigorous comparison of the computational costs of the current and existing algorithms could lead to potential insights or at the very least, quantification of the computational gains as a result of using the current algorithm.
